{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TP6tUbPWzcI1"
   },
   "source": [
    "Import module printing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "igY0uw9MK39U",
    "outputId": "b672b218-fd6b-4c93-c2e3-1532ef908630"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>@polythenespam I will be all packed for rehab....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Sorry if you at having trouble viewing my stor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Ah! My pc is so bad to me!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>@kstew_fan No shade, still all old!  And you h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Our phone line is dead.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>Spending my last days in Washington with the b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>http://twitpic.com/6ivhj - Demi...√É¬¢√Ç¬ô√Ç¬•√É¬¢√Ç¬ô√Ç¬•...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>@missxseptemberk nope, because then I'd miss m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>@JillzWorth its sooooooooo gone    i feel so a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>lost ma phone... man this is da worst bday wee...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               Text\n",
       "0      1  @polythenespam I will be all packed for rehab....\n",
       "1      1  Sorry if you at having trouble viewing my stor...\n",
       "2      1                      Ah! My pc is so bad to me!!! \n",
       "3      1  @kstew_fan No shade, still all old!  And you h...\n",
       "4      1                           Our phone line is dead. \n",
       "5      1  Spending my last days in Washington with the b...\n",
       "6      0  http://twitpic.com/6ivhj - Demi...√É¬¢√Ç¬ô√Ç¬•√É¬¢√Ç¬ô√Ç¬•...\n",
       "7      1  @missxseptemberk nope, because then I'd miss m...\n",
       "8      0  @JillzWorth its sooooooooo gone    i feel so a...\n",
       "9      1  lost ma phone... man this is da worst bday wee..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#READ DATA\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\USER\\\\Desktop\\\\New Research\\\\dataset.csv\")\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xXZPCzUoNpHR"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn import metrics\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q-bGeMhvOPRj",
    "outputId": "598b6cde-7e33-4387-835d-0768dc6ac74d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128037    @littleangel93 I think i'm falling in love wit...\n",
      "5192      looking for recipes for diabetics, that actual...\n",
      "50057                                 @danbimrose anytime! \n",
      "109259    Todays shift at work is 10:30 - 7:15  I called...\n",
      "73349                       Where are all ma girls tonight \n",
      "                                ...                        \n",
      "31132     hix,.... good bye my friends, good bye School ...\n",
      "42761     @tracieshu its super cute. we watched it after...\n",
      "120772    @courtneyyy3 i know. you're missing late night...\n",
      "49752     @tommcfly hey tom  say hi to me, please please...\n",
      "48941     What does a man have to do to get a freaking g...\n",
      "Name: Text, Length: 150000, dtype: object\n",
      "128037    0\n",
      "5192      1\n",
      "50057     0\n",
      "109259    1\n",
      "73349     1\n",
      "         ..\n",
      "31132     1\n",
      "42761     1\n",
      "120772    1\n",
      "49752     1\n",
      "48941     1\n",
      "Name: label, Length: 150000, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "\n",
    "sample_data = data.sample(150000,replace=True, random_state=1)\n",
    "X = sample_data['Text']\n",
    "y = sample_data['label']\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dV1xQ5vaDmkb"
   },
   "source": [
    "# STOP WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oEl3Gwx4Nu_7",
    "outputId": "3a418e91-bb0d-4b77-faf0-6da0c144cae8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CDuAbe--Drdj"
   },
   "source": [
    "# CLEAN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vxVbkpztN1wa",
    "outputId": "12d67277-f4de-40c3-a670-c7af7146175a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common POS tags in negative words:\n",
      "[('NN', 279060), ('JJ', 98524), ('VB', 51556), ('NNS', 50896), ('VBP', 39925), ('VBG', 37556), ('VBD', 28735), ('RB', 26769), ('VBN', 19943), ('VBZ', 13296)]\n",
      "    label                                               Text  \\\n",
      "0       1  @polythenespam I will be all packed for rehab....   \n",
      "1       1  Sorry if you at having trouble viewing my stor...   \n",
      "2       1                      Ah! My pc is so bad to me!!!    \n",
      "3       1  @kstew_fan No shade, still all old!  And you h...   \n",
      "4       1                           Our phone line is dead.    \n",
      "5       1  Spending my last days in Washington with the b...   \n",
      "6       0  http://twitpic.com/6ivhj - Demi...√É¬¢√Ç¬ô√Ç¬•√É¬¢√Ç¬ô√Ç¬•...   \n",
      "7       1  @missxseptemberk nope, because then I'd miss m...   \n",
      "8       0  @JillzWorth its sooooooooo gone    i feel so a...   \n",
      "9       1  lost ma phone... man this is da worst bday wee...   \n",
      "10      0  @revjesse @TipSquirrel Haha... if I ruled the ...   \n",
      "11      1  Not been tweeting much  Resi was good, went to...   \n",
      "12      0  @davidtoddmke @digtlhippie (AKA Andy) is sad b...   \n",
      "13      1  Haha. I was super sick all day..and I couldn't...   \n",
      "14      1  In my garden, working on my tan and trying to ...   \n",
      "15      0     @lenoreva Happy Birthday! Have an awesome one    \n",
      "16      1        @awesomenessjen LOL! I have tutoring soon.    \n",
      "17      0       finals=over, SAT's=over, SUMMER= just began    \n",
      "18      1  chantelle's going to formula 1 nd leaving jess...   \n",
      "19      1  this week is really weird. i want it to just end    \n",
      "\n",
      "                                     Cleaned_Text_POS  \\\n",
      "0   [(pack, VBN), (rehab, NN), (tell, VB), (date, ...   \n",
      "1   [(sorry, NN), (trouble, NN), (view, VBG), (sto...   \n",
      "2                     [(ah, VB), (pc, NN), (bad, JJ)]   \n",
      "3   [(shade, NN), (old, JJ), (new, JJ), (pesquisar...   \n",
      "4               [(phone, NN), (line, NN), (dead, JJ)]   \n",
      "5   [(spending, NN), (day, NNS), (washington, NN),...   \n",
      "6   [(demi, NN), (hope, VBP), (come, VBZ), (german...   \n",
      "7   [(nope, NN), (iwould, JJ), (miss, VB), (wifey,...   \n",
      "8   [(sooooooooo, NN), (go, VBN), (feel, VBP), (aw...   \n",
      "9   [(lose, VBN), (phone, NN), (man, NN), (da, JJ)...   \n",
      "10  [(haha, NN), (rule, VBD), (world, NN), (therew...   \n",
      "11  [(tweet, VBG), (resi, NN), (good, JJ), (go, VB...   \n",
      "12  [(aka, NN), (andy, NN), (sad, JJ), (find, VB),...   \n",
      "13  [(haha, NN), (super, JJ), (sick, JJ), (dayand,...   \n",
      "14  [(garden, NN), (work, VBG), (tan, NN), (try, V...   \n",
      "15       [(happy, JJ), (birthday, NN), (awesome, JJ)]   \n",
      "16              [(lol, NN), (tutor, VBG), (soon, RB)]   \n",
      "17  [(finalsover, NN), (satisover, RB), (summer, N...   \n",
      "18  [(chantelleis, NN), (go, VBG), (formula, VB), ...   \n",
      "19  [(week, NN), (weird, JJ), (want, VBP), (end, VB)]   \n",
      "\n",
      "                                         Cleaned_Text  \n",
      "0        pack rehab tell date bruno twittersober lmao  \n",
      "1            sorry trouble view story site tech issue  \n",
      "2                                           ah pc bad  \n",
      "3                             shade old new pesquisar  \n",
      "4                                     phone line dead  \n",
      "5   spending day washington best th grade leader p...  \n",
      "6                    demi hope come germanywanna live  \n",
      "7                           nope iwould miss wifey lt  \n",
      "8                          sooooooooo go feel awesome  \n",
      "9   lose phone man da worst bday weekend everi wis...  \n",
      "10         haha rule world therewould hectic nonsense  \n",
      "11  tweet resi good go friday th bday party lastni...  \n",
      "12                         aka andy sad find big twit  \n",
      "13  haha super sick dayand hotel bad french experi...  \n",
      "14  garden work tan try find way think ruin quotho...  \n",
      "15                             happy birthday awesome  \n",
      "16                                     lol tutor soon  \n",
      "17                  finalsover satisover summer begin  \n",
      "18      chantelleis go formula nd leave jess saturday  \n",
      "19                                week weird want end  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as spacy_stopwords\n",
    "from collections import Counter\n",
    "\n",
    "# Download necessary NLTK datasets\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Stopwords set (combining NLTK and spaCy stopwords)\n",
    "stop_words = frozenset(stopwords.words('english')).union(spacy_stopwords)\n",
    "\n",
    "# Dictionary of contractions and their expanded forms\n",
    "abbreviation_map = {\n",
    "    \"isn't\": \"is not\", \"aren't\": \"are not\", \"can't\": \"cannot\",\n",
    "    \"couldn't\": \"could not\", \"don't\": \"do not\", \"doesn't\": \"does not\",\n",
    "    \"n't\": \"not\", \"'re\": \"are\", \"'ve\": \"have\", \"'ll\": \"will\",\n",
    "    \"'d\": \"would\", \"'s\": \"is\"\n",
    "}\n",
    "\n",
    "# Enhanced data cleaning function\n",
    "def clean_text(text):\n",
    "    # Convert to lowercase and handle abbreviations\n",
    "    text = text.lower()\n",
    "    for short_form, full_form in abbreviation_map.items():\n",
    "        text = re.sub(rf\"\\b{re.escape(short_form)}\\b\", full_form, text)\n",
    "    \n",
    "    # Combine all regex replacements into a single call\n",
    "    text = re.sub(\n",
    "        r'http\\S+|www\\S+|https\\S+|'        # URLs\n",
    "        r'\\b\\w+@\\w+\\.\\w+\\b|'               # Emails\n",
    "        r'@\\w+|#\\w+|'                      # Hashtags and mentions\n",
    "        r'\\d+|'                            # Digits\n",
    "        r'[^\\x00-\\x7F]+|'                  # Non-English symbols\n",
    "        r'\\s+', ' ',                       # Excessive whitespace\n",
    "        text\n",
    "    )\n",
    "    \n",
    "    # Remove punctuation and trim whitespace\n",
    "    return text.translate(str.maketrans('', '', string.punctuation)).strip()\n",
    "\n",
    "# Tokenize, lemmatize, and filter by POS and stopwords in one step\n",
    "def process_and_filter_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tagged = pos_tag(tokens)\n",
    "    filtered_words = []\n",
    "    \n",
    "    for word, tag in pos_tagged:\n",
    "        # Exclude stopwords and unwanted POS tags (e.g., prepositions and conjunctions)\n",
    "        if word in stop_words or tag in {'IN', 'CC'}:\n",
    "            continue\n",
    "        \n",
    "        # Determine POS and lemmatize accordingly\n",
    "        if tag.startswith('NN'):\n",
    "            lemma = lemmatizer.lemmatize(word, pos='n')\n",
    "        elif tag.startswith('VB'):\n",
    "            lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        elif tag.startswith('JJ'):\n",
    "            lemma = lemmatizer.lemmatize(word, pos='a')\n",
    "        elif tag.startswith('RB'):\n",
    "            lemma = lemmatizer.lemmatize(word, pos='r')\n",
    "        else:\n",
    "            lemma = lemmatizer.lemmatize(word)\n",
    "        \n",
    "        filtered_words.append((lemma, tag))  # Append word with POS tag if it passes all filters\n",
    "    \n",
    "    return filtered_words\n",
    "\n",
    "# Full data cleaning and preprocessing pipeline\n",
    "def preprocess_data(text):\n",
    "    cleaned_text = clean_text(text)\n",
    "    return process_and_filter_text(cleaned_text)\n",
    "\n",
    "# Apply the preprocessing function to the dataset\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\USER\\\\Desktop\\\\New Research\\\\dataset.csv\")\n",
    "data['Cleaned_Text_POS'] = data['Text'].apply(preprocess_data)\n",
    "\n",
    "# Filter negative samples and analyze POS tags\n",
    "negative_data = data[data['label'] == 1]\n",
    "\n",
    "# Collect POS tags of words in negative texts\n",
    "pos_tags_negative = Counter()\n",
    "for row in negative_data['Cleaned_Text_POS']:\n",
    "    pos_tags_negative.update(tag for _, tag in row)\n",
    "\n",
    "# Display the most common POS tags associated with negative words\n",
    "print(\"Most common POS tags in negative words:\")\n",
    "print(pos_tags_negative.most_common(10))\n",
    "\n",
    "# Remove the POS tags to retain only the cleaned text for other uses\n",
    "data['Cleaned_Text'] = data['Cleaned_Text_POS'].apply(lambda x: ' '.join(word for word, _ in x))\n",
    "\n",
    "# Display the first 20 rows of the cleaned dataset with POS analysis\n",
    "print(data.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JcwcliSDt1dN",
    "outputId": "b7c427e7-7336-4002-ddde-7bc2ec37e906"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(149900, 4)\n",
      "   label                                               Text  \\\n",
      "0      1  @polythenespam I will be all packed for rehab....   \n",
      "1      1  Sorry if you at having trouble viewing my stor...   \n",
      "2      1                      Ah! My pc is so bad to me!!!    \n",
      "3      1  @kstew_fan No shade, still all old!  And you h...   \n",
      "4      1                           Our phone line is dead.    \n",
      "\n",
      "                                    Cleaned_Text_POS  \\\n",
      "0  [(pack, VBN), (rehab, NN), (tell, VB), (date, ...   \n",
      "1  [(sorry, NN), (trouble, NN), (view, VBG), (sto...   \n",
      "2                    [(ah, VB), (pc, NN), (bad, JJ)]   \n",
      "3  [(shade, NN), (old, JJ), (new, JJ), (pesquisar...   \n",
      "4              [(phone, NN), (line, NN), (dead, JJ)]   \n",
      "\n",
      "                                   Cleaned_Text  \n",
      "0  pack rehab tell date bruno twittersober lmao  \n",
      "1      sorry trouble view story site tech issue  \n",
      "2                                     ah pc bad  \n",
      "3                       shade old new pesquisar  \n",
      "4                               phone line dead  \n",
      "Index(['label', 'Text', 'Cleaned_Text_POS', 'Cleaned_Text'], dtype='object')\n",
      "128037    @littleangel93 I think i'm falling in love wit...\n",
      "5192      looking for recipes for diabetics, that actual...\n",
      "50057                                 @danbimrose anytime! \n",
      "109259    Todays shift at work is 10:30 - 7:15  I called...\n",
      "73349                       Where are all ma girls tonight \n",
      "Name: Text, dtype: object\n",
      "128037    0\n",
      "5192      1\n",
      "50057     0\n",
      "109259    1\n",
      "73349     1\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data.head()\n",
    "data.shape\n",
    "print(data.shape)\n",
    "print(data.head())\n",
    "print(data.columns)\n",
    "print(X.head())\n",
    "print(y.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0PEbnopZKYMS"
   },
   "source": [
    "# DISTILBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SIw11-SuBaiV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Roaming\\Python\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\USER\\AppData\\Roaming\\Python\\Python310\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\USER\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "  2%|‚ñè         | 100/4500 [12:04<12:52:22, 10.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.522, 'grad_norm': 4.671262741088867, 'learning_rate': 4.888888888888889e-05, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñç         | 200/4500 [21:05<2:26:36,  2.05s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4661, 'grad_norm': 5.349675178527832, 'learning_rate': 4.7777777777777784e-05, 'epoch': 0.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|‚ñã         | 300/4500 [25:01<2:46:17,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4408, 'grad_norm': 5.4919753074646, 'learning_rate': 4.666666666666667e-05, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|‚ñâ         | 400/4500 [29:02<2:45:22,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4241, 'grad_norm': 3.5491783618927, 'learning_rate': 4.555555555555556e-05, 'epoch': 0.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|‚ñà         | 500/4500 [32:54<2:15:11,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4233, 'grad_norm': 2.4339427947998047, 'learning_rate': 4.4444444444444447e-05, 'epoch': 0.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|‚ñà‚ñé        | 600/4500 [36:49<2:34:02,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4375, 'grad_norm': 2.7152974605560303, 'learning_rate': 4.3333333333333334e-05, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|‚ñà‚ñå        | 700/4500 [40:46<2:27:49,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3988, 'grad_norm': 5.303695201873779, 'learning_rate': 4.222222222222222e-05, 'epoch': 0.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|‚ñà‚ñä        | 800/4500 [44:38<2:04:16,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4322, 'grad_norm': 4.579912185668945, 'learning_rate': 4.111111111111111e-05, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|‚ñà‚ñà        | 900/4500 [48:30<2:22:43,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3856, 'grad_norm': 5.9063286781311035, 'learning_rate': 4e-05, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|‚ñà‚ñà‚ñè       | 1000/4500 [52:26<2:17:14,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3592, 'grad_norm': 5.041999816894531, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|‚ñà‚ñà‚ñç       | 1100/4500 [56:20<1:59:13,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4091, 'grad_norm': 4.6959967613220215, 'learning_rate': 3.777777777777778e-05, 'epoch': 0.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|‚ñà‚ñà‚ñã       | 1200/4500 [1:00:06<2:07:54,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.383, 'grad_norm': 3.028545379638672, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|‚ñà‚ñà‚ñâ       | 1300/4500 [1:03:59<2:03:16,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3769, 'grad_norm': 7.151299953460693, 'learning_rate': 3.555555555555556e-05, 'epoch': 0.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|‚ñà‚ñà‚ñà       | 1400/4500 [1:07:52<2:01:01,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3789, 'grad_norm': 3.37174654006958, 'learning_rate': 3.444444444444445e-05, 'epoch': 0.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|‚ñà‚ñà‚ñà‚ñé      | 1500/4500 [1:11:38<1:58:50,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3937, 'grad_norm': 3.4359235763549805, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 33%|‚ñà‚ñà‚ñà‚ñé      | 1500/4500 [1:14:51<1:58:50,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.37181323766708374, 'eval_accuracy': 0.8418333333333333, 'eval_f1': 0.8400288903702121, 'eval_precision': 0.8394204801495673, 'eval_recall': 0.8418333333333333, 'eval_runtime': 192.7918, 'eval_samples_per_second': 31.122, 'eval_steps_per_second': 3.89, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|‚ñà‚ñà‚ñà‚ñå      | 1600/4500 [1:18:41<1:49:39,  2.27s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.276, 'grad_norm': 7.614543437957764, 'learning_rate': 3.222222222222223e-05, 'epoch': 1.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|‚ñà‚ñà‚ñà‚ñä      | 1700/4500 [1:22:19<1:46:04,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2531, 'grad_norm': 3.318570852279663, 'learning_rate': 3.111111111111111e-05, 'epoch': 1.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|‚ñà‚ñà‚ñà‚ñà      | 1800/4500 [1:26:08<1:42:11,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2527, 'grad_norm': 6.53172492980957, 'learning_rate': 3e-05, 'epoch': 1.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1900/4500 [1:29:55<1:38:41,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2532, 'grad_norm': 7.192573070526123, 'learning_rate': 2.8888888888888888e-05, 'epoch': 1.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 2000/4500 [1:33:33<1:24:55,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2622, 'grad_norm': 4.810956001281738, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 2100/4500 [1:37:21<1:31:23,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2521, 'grad_norm': 4.379487991333008, 'learning_rate': 2.6666666666666667e-05, 'epoch': 1.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 2200/4500 [1:41:08<1:26:44,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2553, 'grad_norm': 6.138952255249023, 'learning_rate': 2.5555555555555554e-05, 'epoch': 1.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2300/4500 [1:44:52<1:11:31,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2711, 'grad_norm': 7.555908679962158, 'learning_rate': 2.4444444444444445e-05, 'epoch': 1.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 2400/4500 [1:48:31<1:18:52,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2626, 'grad_norm': 5.9534735679626465, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 2500/4500 [1:52:17<1:15:05,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2617, 'grad_norm': 5.795188903808594, 'learning_rate': 2.2222222222222223e-05, 'epoch': 1.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 2600/4500 [1:56:01<1:10:46,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2676, 'grad_norm': 8.378863334655762, 'learning_rate': 2.111111111111111e-05, 'epoch': 1.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 2700/4500 [1:59:37<1:06:50,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2492, 'grad_norm': 9.075489044189453, 'learning_rate': 2e-05, 'epoch': 1.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2800/4500 [2:03:20<1:02:42,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2457, 'grad_norm': 5.633772850036621, 'learning_rate': 1.888888888888889e-05, 'epoch': 1.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2900/4500 [2:07:02<58:32,  2.20s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2542, 'grad_norm': 5.2645649909973145, 'learning_rate': 1.777777777777778e-05, 'epoch': 1.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 3000/4500 [2:10:34<54:27,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2486, 'grad_norm': 7.1342339515686035, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 3000/4500 [2:13:44<54:27,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4252893924713135, 'eval_accuracy': 0.8415, 'eval_f1': 0.8409474015879878, 'eval_precision': 0.8405177995552038, 'eval_recall': 0.8415, 'eval_runtime': 189.0363, 'eval_samples_per_second': 31.74, 'eval_steps_per_second': 3.967, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 3100/4500 [2:17:26<51:37,  2.21s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1297, 'grad_norm': 17.560457229614258, 'learning_rate': 1.5555555555555555e-05, 'epoch': 2.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 3200/4500 [2:21:08<48:04,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.126, 'grad_norm': 1.8915683031082153, 'learning_rate': 1.4444444444444444e-05, 'epoch': 2.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 3300/4500 [2:24:15<32:58,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1337, 'grad_norm': 0.39480674266815186, 'learning_rate': 1.3333333333333333e-05, 'epoch': 2.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3400/4500 [2:26:21<22:59,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.136, 'grad_norm': 14.017953872680664, 'learning_rate': 1.2222222222222222e-05, 'epoch': 2.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 3500/4500 [2:28:27<21:00,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1583, 'grad_norm': 1.90432870388031, 'learning_rate': 1.1111111111111112e-05, 'epoch': 2.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 3600/4500 [2:30:32<18:50,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.123, 'grad_norm': 32.2635383605957, 'learning_rate': 1e-05, 'epoch': 2.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 3700/4500 [2:32:38<16:43,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1274, 'grad_norm': 13.596877098083496, 'learning_rate': 8.88888888888889e-06, 'epoch': 2.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 3800/4500 [2:34:43<14:32,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1446, 'grad_norm': 0.16880196332931519, 'learning_rate': 7.777777777777777e-06, 'epoch': 2.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 3900/4500 [2:36:49<12:34,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1656, 'grad_norm': 12.235422134399414, 'learning_rate': 6.666666666666667e-06, 'epoch': 2.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 4000/4500 [2:38:54<10:29,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1727, 'grad_norm': 23.148046493530273, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 4100/4500 [2:40:59<08:16,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1474, 'grad_norm': 1.0926319360733032, 'learning_rate': 4.444444444444445e-06, 'epoch': 2.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 4200/4500 [2:43:05<06:16,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1104, 'grad_norm': 0.11351057887077332, 'learning_rate': 3.3333333333333333e-06, 'epoch': 2.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 4300/4500 [2:45:10<04:11,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1004, 'grad_norm': 37.72224044799805, 'learning_rate': 2.2222222222222225e-06, 'epoch': 2.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 4400/4500 [2:47:16<02:05,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1399, 'grad_norm': 15.433979034423828, 'learning_rate': 1.1111111111111112e-06, 'epoch': 2.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4500/4500 [2:49:22<00:00,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1265, 'grad_norm': 0.2914012670516968, 'learning_rate': 0.0, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4500/4500 [2:51:17<00:00,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6782447099685669, 'eval_accuracy': 0.8411666666666666, 'eval_f1': 0.838793386726796, 'eval_precision': 0.8383921383168066, 'eval_recall': 0.8411666666666666, 'eval_runtime': 115.1127, 'eval_samples_per_second': 52.123, 'eval_steps_per_second': 6.515, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4500/4500 [2:51:18<00:00,  2.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 10278.4266, 'train_samples_per_second': 7.005, 'train_steps_per_second': 0.438, 'train_loss': 0.2697330820295546, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 750/750 [01:54<00:00,  6.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBERT Evaluation Accuracy: 0.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Optional: Disable W&B logging\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# Load your cleaned dataset\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\USER\\\\Desktop\\\\New Research\\\\dataset.csv\")\n",
    "\n",
    "# Strip whitespace from column names\n",
    "data.columns = data.columns.str.strip()\n",
    "\n",
    "# Ensure 'Text' and 'label' columns exist\n",
    "if 'Text' not in data.columns or 'label' not in data.columns:\n",
    "    raise KeyError(\"Columns 'Text' or 'label' not found in the dataset.\")\n",
    "\n",
    "# Fill NaN values and ensure all entries are strings\n",
    "data['Text'] = data['Text'].fillna('').astype(str)\n",
    "\n",
    "# Sample the data if it has more than 30,000 entries\n",
    "if len(data) > 30000:\n",
    "    data = data.sample(n=30000, random_state=42)\n",
    "\n",
    "# Prepare the DistilBERT tokenizer and model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased', \n",
    "    num_labels=len(data['label'].unique())\n",
    ")\n",
    "\n",
    "# Prepare the input texts and labels\n",
    "texts = data['Text'].tolist()\n",
    "labels = data['label'].astype('category').cat.codes.tolist()\n",
    "\n",
    "# Split data into training and evaluation sets\n",
    "train_texts, eval_texts, train_labels, eval_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize the texts\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
    "eval_encodings = tokenizer(eval_texts, truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# Create a custom torch dataset\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['label'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = CustomDataset(train_encodings, train_labels)\n",
    "eval_dataset = CustomDataset(eval_encodings, eval_labels)\n",
    "\n",
    "# Set training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./distilbert_results',\n",
    "    run_name='my_distilbert_experiment',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=5e-5,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "# Define a compute_metrics function to calculate accuracy and other metrics\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# Define Trainer with compute_metrics function\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics  # Include compute_metrics here\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model(\"distilbert_model\")\n",
    "\n",
    "# Evaluate the model\n",
    "eval_metrics = trainer.evaluate()\n",
    "if 'eval_accuracy' in eval_metrics:\n",
    "    print(f\"DistilBERT Evaluation Accuracy: {eval_metrics['eval_accuracy']:.2f}\")\n",
    "else:\n",
    "    print(\"Accuracy not found in evaluation metrics.\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
